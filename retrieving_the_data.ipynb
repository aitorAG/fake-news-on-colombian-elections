{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving the data from Twitter\n",
    "\n",
    "This were the steps taken to retrieve data from Twitter, mentioning the candidates Iván Duque and Gustavo Petro.\n",
    "\n",
    "This was done using Twitter's HTTP API with the requests package.\n",
    "\n",
    "You can apply for a developer account [here](https://developer.twitter.com/en/apply-for-access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "consumer_key = \"\" # Set this to your consumer key\n",
    "consumer_secret = \"\" # Set this to your consumer secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Twitter API\n",
    "\n",
    "There are Python packages that allow access to Twitter's APIs, however, for this project, they will be left aside, so that the data can be used exactly in the way it's needed.\n",
    "\n",
    "Therefore, the `requests` package will be used to perform the requests.\n",
    "\n",
    "### Authenticating requests\n",
    "\n",
    "Twitter requires that the requests to its APIs are authenticated. The whole authentication process is described in detail [here](https://developer.twitter.com/en/docs/basics/authentication/overview/application-only), but it basically consists of 2 steps:\n",
    "\n",
    "1. Base 64 encoding the consumer key and secret\n",
    "2. Obtaining an access token from Twitter's Oauth2 endpoint\n",
    "\n",
    "So, here's a very simple way to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded = base64.b64encode(\"%s:%s\" % (consumer_key, consumer_secret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have succesfully base64 encoded the consumer key and secret.\n",
    "\n",
    "Now, it's necessary to obtain an access token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Authorization': 'Basic ' + encoded, # An auth header needs to be sent, with the encoded string from before\n",
    "    'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8' # This is the required content type\n",
    "}\n",
    "\n",
    "# Twitter's docs explain that this must be the body sent to the Oauth2 endpoint\n",
    "req_body = 'grant_type=client_credentials'\n",
    "\n",
    "# This is just the URL for the Oauth2 endpoint\n",
    "auth_url = 'https://api.twitter.com/oauth2/token'\n",
    "\n",
    "# We send a POST request to the endpoint, passing the body and headers\n",
    "auth_token_res = requests.post(auth_url, data=req_body, headers=headers)\n",
    "\n",
    "# We extract the response, which is a JSON object\n",
    "auth_object = auth_token_res.json()\n",
    "\n",
    "# We extract the access token from the response\n",
    "access_token = auth_object['access_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there it is!\n",
    "\n",
    "We've succesfully gotten an auth token that we'll use for requesting the data from now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to start making requests to Twitter's API, and since we'll be using the same auth token and URL, we'll set those next\n",
    "\n",
    "### Twitter's API URLs\n",
    "\n",
    "Twitter has an interesting way for generating query endpoints for users to access their API. It looks like this:\n",
    "\n",
    "```\n",
    "                                               And this label refers to the label you chose\n",
    " This looks the same for everybody              for your environment. Mine is 'Development'\n",
    "|----------------------------------------|            |\n",
    "https://api.twitter.com/1.1/tweets/search/:product/:label.json\n",
    "                                              |\n",
    "                        This refers to the \"product\". e.g. 30day or fullarchive\n",
    "                                     in the case of search\n",
    "```\n",
    "\n",
    "For the auth token, we'll use an `Authorization` header with a value of `\"Bearer <token>\"`, where `<token>` is of course, the access token we obtained before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_api_url = 'https://api.twitter.com/1.1/tweets/search/30day/Development.json'\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Bearer ' + access_token\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requesting data from the API\n",
    "\n",
    "Now, we can start issuing GET requests to the endpoint, adjusting the `'query'` param according to our needs.\n",
    "For the case of this project, only two things will be included in the query, a \"mention\" of the user we'll be targeting, and we would also like to exclude retweets.\n",
    "\n",
    "The \"mention\" part is pretty straightforward, it only needs to be included in the query with an '@' sign.\n",
    "\n",
    "Twitter provides a couple of ways for excluding retweets, namely, you can use `is:retweet` or `filter:retweet`, which you would need to include with a \"`-`\" sign to specify negation. Like `-is:retweet`.\n",
    "\n",
    "However, those operators are not available on the Sandbox version, which is the free version of premium endpoints, like the full archive search or 30 day search ones. So, to workaround this issue, we can negate the exact match \"RT\", because we know that all retweets will include this right at the start. This might lead to a couple of false negatives, in case a user had \"RT\" as part of their tweet, but since the text of tweets is tokenized, we don't have to worry about tweets including words like \"ART\" being ommitted, because RT will be searched as a whole \"word\".\n",
    "\n",
    "Also, the fromDate and toDate params will be used when getting the actual data for the project, to specify the dates of the electoral campaigns\n",
    "\n",
    "With all that said, we can now build our query and start getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'query': '@IvanDuque -\"RT\"'\n",
    "}\n",
    "\n",
    "data = requests.get(base_api_url, headers=headers, params=params)\n",
    "\n",
    "data = data.json()\n",
    "\n",
    "results = data['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've gotten quite a good amount of information. Each of these requests will return 100 tweets, and there is a `'next'`, field, which contains a token we can use for retrieving the next 100 tweets for the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data limitations\n",
    "\n",
    "With the sandbox version of the API, we're limited to only 50 requests to the full archive endpoint, with 100 tweets each. Since the 30day allows some more (250), we'll use it for testing.\n",
    "\n",
    "For this project, we're particularly interested in the period of the 2018 electoral campaign in Colombia. This campaigns ran from january 27, 2018, to may 27, 2018 for the first round of voting. And for the second one, from may 27, 2018, to june 17, 2018. So in general, we're interested in the period between january 27, 2018 and june 17, 2018.\n",
    "\n",
    "That leaves us with 5 months of data we're interested in. So, considering that we're limited to 50 requests, it would be fine to split those equally among the relevant months.\n",
    "\n",
    "We would also like to focus on two candidates in particular: Iván Duque, the elected president of Colombia, and Gustavo Petro, his most noticeable opponent. So, it would be okay to leave half of the monthly requests to each one of the candidates.\n",
    "\n",
    "To summarize, here's how we'll split our requests:\n",
    "\n",
    "|Date / N° of requests  | Gustavo Petro | Iván Duque |\n",
    "|-----------------------| ------------- | ---------- |\n",
    "|2018-01-27 - 2018-02-27| 5             | 5          |\n",
    "|2018-02-27 - 2018-03-27| 5             | 5          |\n",
    "|2018-03-27 - 2018-04-27| 5             | 5          |\n",
    "|2018-04-27 - 2018-05-27| 5             | 5          |\n",
    "|2018-05-27 - 2018-06-17| 5             | 5          |\n",
    "|total                  | 25            | 25         |\n",
    "\n",
    "This will leave us with a total of 2500 tweets about each candidate\n",
    "\n",
    "However, since for now we're only testing, we will use a slightly different variation, working with the 30day index.\n",
    "\n",
    "So, the idea here will be to split the last month in five intervals, and request data at least twice for only one of the candidates. This way we can mimic the process we'll be doing with the data we really want. So let's get to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll import a couple of utilities to get the dates as necessary\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# We set an initial date 30 days ago, and shift it 6 days at a time to get the intervals\n",
    "current_date = datetime.now() - timedelta(days=30)\n",
    "intervals = []\n",
    "\n",
    "for i in range(0, 5):\n",
    "    # Set a 'next' date, 6 days from the current one\n",
    "    next_date = current_date + timedelta(days=6)\n",
    "\n",
    "    interval = {\n",
    "        'fromDate': current_date.strftime(\"%Y%m%d\") + '0000',\n",
    "        'toDate': next_date.strftime(\"%Y%m%d\") + '0000' # format the dates in the way expected by Twitter\n",
    "    }\n",
    "\n",
    "    intervals.append(interval)\n",
    "    current_date = next_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got the intervals, we can start requesting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def request_data(intervals, search_term, requests_per_interval=2):\n",
    "    results = []\n",
    "    \n",
    "    for interval in intervals:\n",
    "        # Params need to be reset for each interval\n",
    "        params = {'query': search_term + ' -\"RT\"'}\n",
    "        # Add the fromDate and toDate fields to the params\n",
    "        params.update(interval)\n",
    "        \n",
    "        # We perform an initial request to ensure that our result will have a 'next' field.\n",
    "        current_request = requests.get(base_api_url, headers=headers, params=params)\n",
    "        \n",
    "        for i in range(0, requests_per_interval):\n",
    "            data = current_request.json()\n",
    "            results += data['results']\n",
    "            if i + 1 < requests_per_interval:\n",
    "                # We add the next param, to fetch the next \"page\" of results\n",
    "                params['next'] = data['next']\n",
    "                current_request = requests.get(base_api_url, headers=headers, params=params)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we fetch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = request_data(intervals, '@IvanDuque')\n",
    "\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We've now gotten 1000 tweets we can store in a JSON file or other type of file, so that we can later process it and adjust it to our needs\n",
    "\n",
    "## Exporting the tweets\n",
    "\n",
    "We'll now save this data into a new JSON file, to use it in the other parts of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# the json package provides a simple dump function to encode data as json\n",
    "with open('data/test_data.json', 'w') as f:\n",
    "    json.dump(test_data, f) # here, the json dump is being sent directly to the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done. We now have a file that contains all of our downloaded data.\n",
    "\n",
    "# Working with the actual data\n",
    "\n",
    "Now we're more than ready to repeat the process with the data we're really going to use in our project.\n",
    "\n",
    "We'll still use the headers from before, with the same auth token. So that doesn't need to be changed.\n",
    "\n",
    "The endpoint for the full archive, however, is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You'll notice that my environment here is called 'development' with a lowercase 'd'\n",
    "# and that the endpoint for the full archive is just 'fullarchive'\n",
    "\n",
    "base_api_url = 'https://api.twitter.com/1.1/tweets/search/fullarchive/development.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, for this endpoint, the approach we'll use with the dates will be a little bit different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the relevant dates in our time period of interest\n",
    "dates = [\"201801270000\", \"201802270000\", \"201803270000\", \"201804270000\", \"201805270000\", \"201806172359\"]\n",
    "\n",
    "# Produce intervals between one date and the next\n",
    "intervals = [{'fromDate': dates[i], 'toDate': dates[i + 1]} for i in range(0, len(dates) - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now request our data just like we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll first request data for the candidate Iván Duque\n",
    "duque_data = request_data(intervals, '@IvanDuque', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ande save it to its own file\n",
    "with open('data/duque_data.json', 'w') as f:\n",
    "    json.dump(duque_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And now we'll request the data for the candidate Gustavo Petro\n",
    "petro_data = request_data(intervals, '@petrogustavo', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And we save it to its own file as well\n",
    "with open('data/petro_data.json', 'w') as f:\n",
    "    json.dump(petro_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And finally, we'll concatenate both lists and store the result in a single file\n",
    "all_data = duque_data + petro_data\n",
    "\n",
    "with open('data/data.json', 'w') as f:\n",
    "    json.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it. Our data files are now ready to be used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
